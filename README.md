# ðŸ”° Varuna AI â€“ Secure RAG Chat Assistant (Air-Gapped)

Varuna AI is a secure, offline-capable **Retrieval-Augmented Generation (RAG)** assistant built for air-gapped environments. Leveraging **LLaMA 3.1 8B**, FAISS, OCR, BLIP, and LangChain, it enables **document-grounded AI chat** across PDFs, Word docs, tables, and image-based inputs.

> ðŸ“ Developed for air-gapped on-premise use-cases; optimized for secure deployments with no internet dependency. All components run in isolated environments using `llama.cpp`, `FastAPI`, `React`, and `NGINX`.

---

## ðŸ§  Key Features

- ðŸ”’ **Air-Gapped Ready**: Fully offline-capable, no external API or internet dependency  
- ðŸ“„ **Multimodal RAG**: Document, image, OCR, and table-based information retrieval  
- ðŸ§  **LLM-Powered Chat**: Fast CPU-based inference using quantized LLaMA 3.1 8B  
- âš™ï¸ **End-to-End RAG Pipeline**: From document upload â†’ chunking â†’ embeddings â†’ vector DB â†’ LLM query response  
- ðŸ§¾ **Visual Document Parsing**: BLIP captioning + OCR (EasyOCR)  
- ðŸŒ **Intuitive UI**: Built with ReactJS + Tailwind; session-based chat with multi-document support  
- ðŸ›¡ï¸ **User Authentication**: Secure login, session ID management, document isolation  
- ðŸ“Š **Scalable Microservices**: Embedding, inference, chat API, and UI run as independent services  

---

## ðŸ§± System Architecture

### ðŸ§  RAG Pipeline

![RAG Pipeline](./assets/RAG_Architecture.PNG)

1. **Docs** parsed and chunked  
2. **Embeddings** generated and stored in FAISS  
3. **User query** is embedded and searched  
4. Retrieved chunks passed to **LLM** for final response generation  

---

### ðŸ” Full Deployment Architecture

![Varuna Architecture](./assets/full-architecture.png)

| Component       | Description                             |
|----------------|-----------------------------------------|
| `Frontend`      | React (Port: 3000)                      |
| `Backend`       | FastAPI (Port: 8000)                    |
| `Inference`     | LLaMA 3.1 (Port: 8081 via `llama.cpp`)  |
| `Embeddings`    | `nomic-embed-text` (Port: 8083)         |
| `OCR`           | EasyOCR                                 |
| `Image Caption` | BLIP (for images)                       |
| `Gateway`       | NGINX reverse proxy (Port: 80)          |
| `Storage`       | SQLite, Session store, FAISS            |

---

## ðŸ—‚ï¸ Repository Structure

```bash
.
â”œâ”€â”€ backend/                 # FastAPI backend - core logic
â”‚   â”œâ”€â”€ main.py              # Auth + Inference APIs
â”‚   â”œâ”€â”€ users.db             # SQLite DB
â”‚   â””â”€â”€ ...                 
â”œâ”€â”€ frontend/                # React frontend
â”‚   â””â”€â”€ /chat /login         # Pages + components
â”œâ”€â”€ models/                  # LLM + embedding model configs
â”‚   â”œâ”€â”€ llama-3.1-8b.gguf
â”‚   â””â”€â”€ nomic-embed-text-v1.gguf
â”œâ”€â”€ llama.cpp/               # LLaMA.cpp binary & compiled engine
â”œâ”€â”€ documentation/           # Diagrams, PDFs, architecture docs
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ get-pip.py               # Setup script
â”œâ”€â”€ startup_services.sh      # Bash script to run services
â””â”€â”€ README.md
```

---

## âš™ï¸ Tech Stack

- **Frontend**: ReactJS, TailwindCSS, Streamlit (combined dashboard)  
- **Backend**: FastAPI, Python, SQLite  
- **LLM**: LLaMA 3.1 8B (Quantized `.gguf`) via `llama.cpp`  
- **Embeddings**: nomic-embed-text-v1 (Q4_K_M)  
- **Vector DB**: FAISS with BM25 + Dense search  
- **OCR/BLIP**: EasyOCR, BLIP model (image captioning)  
- **Deployment**: NGINX reverse proxy, systemd services, bash  

---

## ðŸš€ Local Deployment Guide (Offline)

> âœ… Works fully offline on CPU (no GPU needed)

### 1ï¸âƒ£ Prerequisites

- Python 3.10+  
- NodeJS 18+  
- `llama.cpp` compiled  
- `serve` installed (`npm install -g serve`)  
- NGINX  

---

### 2ï¸âƒ£ Run the Services

```bash
# Backend
cd backend
python3 main.py

# Frontend
cd frontend
npm install
npm run build
PORT=3000 serve -s build

# Inference (Port: 8081)
./llama.cpp/build/bin/llama-server \
  -m models/llama-3.1-8b.Q4_K_M.gguf \
  --port 8081 --chat-template llama3

# Embeddings (Port: 8083)
./llama.cpp/build/bin/llama-server \
  -m models/nomic-embed-text-v1.Q4_K_M.gguf \
  --port 8083 --embeddings

# NGINX (Port: 80)
sudo nginx -c /etc/nginx/nginx.conf
```

---

## ðŸ§ª Core API Endpoints

| Endpoint                         | Method | Functionality                       |
|----------------------------------|--------|-------------------------------------|
| `/register`                      | POST   | New user registration               |
| `/login`                         | POST   | Login user                          |
| `/logout`                        | POST   | Logout session                      |
| `/message/stream`                | POST   | RAG-based chat generation           |
| `/rag/upload/{session_id}`       | POST   | Upload and ingest document          |
| `/rag/session/{id}/tables`       | GET    | Extract tabular data                |
| `/rag/sessions`                  | GET    | View all active sessions            |

---

## ðŸ§¾ Supported Inputs

- PDF, DOCX, TXT files  
- PNG, JPG, JPEG (with OCR + BLIP captioning)  
- Tables (via `pdfplumber` and docx parsers)  

---

## ðŸ“ˆ Optimizations

- ðŸ”¸ Quantized models for fast CPU inference  
- ðŸ”¸ Custom chunking + semantic splitting  
- ðŸ”¸ Parallel OCR + table extraction pipeline  
- ðŸ”¸ Session cache & document reuse  
- ðŸ”¸ Auto-cleanup of idle sessions  

---

## ðŸ“Œ Sample Use Cases

- ðŸ“„ Internal document QA (govt/legal/medical)  
- ðŸ” Confidential chat systems (air-gapped)  
- ðŸ“‘ Legal & financial doc analysis  
- ðŸ§  Private GPT-style assistant with file upload  
- ðŸ›¡ï¸ Air-gapped RAG deployments  

---

## ðŸ›  Future Roadmap

- ðŸ§  Agentic RAG + function calling  
- ðŸ—ƒï¸ Multi-file multi-session querying  
- ðŸ” RBAC (Role-Based Access Control)  
- ðŸ“¦ Docker & Kubernetes deployment  
- ðŸŒ Enterprise dashboard analytics  

---


## ðŸ™Œ Acknowledgements

Built for the Ministry of Defence (Naval R&D) to enable **confidential LLM deployment in offline environments** using fully local components and RAG-based architectures.

---

## Author

> **Shushant Kumar Tiwari**  
> âœ‰ï¸ shushantkumar164@gmail.com  
> ðŸ”— [LinkedIn](https://linkedin.com/in/shushant-tiwari-ai)  
> ðŸ§  AI Engineer | LLMOps | MLOps | RAG
---
